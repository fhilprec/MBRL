%% Master Thesis Table of Contents
%% Integrating Object-Centric Learning with Model-Based Reinforcement Learning

\documentclass[
	english,
	ruledheaders=section,
	class=report,
	thesis={type=master},
	accentcolor=9c,
	custommargins=true,
	marginpar=false,
	parskip=half-,
	fontsize=11pt,
]{tudapub}



\usepackage{iftex}
\ifPDFTeX
	\usepackage[utf8]{inputenc}
\fi



%%%%%%%%%%%%%%%%%%%
% Language settings
%%%%%%%%%%%%%%%%%%%
\usepackage{babel}
\usepackage[autostyle]{csquotes}
\usepackage{microtype}


%%%%%%%%%%%%%%%%%%%
% Bibliography
%%%%%%%%%%%%%%%%%%%
\usepackage{biblatex}
\addbibresource{references.bib}


\DefineBibliographyStrings{english}{
  bibliography = {References},
}


%%%%%%%%%%%%%%%%%%%
% Table packages
%%%%%%%%%%%%%%%%%%%
\usepackage{tabularx}
\usepackage{booktabs}

%%%%%%%%%%%%%%%%%%%
% Math packages
%%%%%%%%%%%%%%%%%%%
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{siunitx}

%%%%%%%%%%%%%%%%%%%
% Additional packages
%%%%%%%%%%%%%%%%%%%
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subcaption}

\begin{document}

\Metadata{
	title=Structured World Understanding: Integrating Object-Centric Learning with Model-Based Reinforcement Learning,
	author=Your Name
}

\title{Structured World Understanding: Integrating Object-Centric Learning with Model-Based Reinforcement Learning}
\subtitle{Master's Thesis: Summer Semester 2025}
\author[Y. Name]{Your Full Name}
\studentID{1234567}
\reviewer{Jannis BlÃ¼ml}

\department{Department of Computer Science}
\institute{Institute of Computer Science}
\group{Machine Learning Research Group}

\submissiondate{24.10.2025}
\examdate{24.10.2025}

\maketitle

\affidavit

\tableofcontents
\listoffigures
\listoftables

% List of Abbreviations
\chapter*{List of Abbreviations}
\addcontentsline{toc}{chapter}{List of Abbreviations}
\begin{tabular}{ll}
	RL   & Reinforcement Learning              \\
	MBRL & Model-Based Reinforcement Learning  \\
	MFRL & Model-Free Reinforcement Learning   \\
	CNN  & Convolutional Neural Network        \\
	LSTM & Long Short-Term Memory              \\
	PPO  & Proximal Policy Optimization        \\
	A3C  & Asynchronous Advantage Actor-Critic \\
	DQN  & Deep Q-Network                      \\
	RSSM & Recurrent State Space Model         \\
	KL   & Kullback-Leibler                    \\
	MSE  & Mean Squared Error                  \\
	RGB  & Red-Green-Blue                      \\
	GPU  & Graphics Processing Unit            \\
	JAX  & Just After eXecution                \\
\end{tabular}

\chapter{Introduction}	
\label{chap:introduction}
RQ1: How do Object Centric World Models compare to pixel-based world models regarding prediction accuracy and sample efficiency?
RQ2: How effectively can the DreamerV2-style actor-critic framework learn optimal policies when trained on imagined rollouts from object-centric world models?
RQ3: What is the impact of different world model architectures (LSTM-based vs. alternatives) on the overall performance of object-centric model-based RL in structured environments?
RQ4: Does the Object Centric inductive bias in WMs improve robustness?

Reinforcement Learning as one of the three categories in Machine Learning among Supervised and Unsupervised Learning follows the approach of having an agent performs actions in an environment and receiving rewards
for certain actions. So unlike Supervised Learning for example there is no ground truth from the which the agent can learn from. Among numerous distinctions within Reinforcement Learning there is the difference 
between Model Based Reinforcement Learning and Model Free Reinforcement Learning. The Latter focuses on the learning of only the Agent which is deployed in the environment (or learns from samples). Thus the agent
will have no explicit way of predicting the next state of the environment, although sufficiently large for example neural networks might create some implicit model of the world. Model based Reinforcement learning 
augments the architecture by a Model that is predicting the next state of the world but also still trains an agent. So there are usually two separate models. That might improve sample efficiency since once the world model
is trained it can be used to train the agent without any need of additional sampling in the environment. Another hope of Model based approaches is that they can achiever a better understanding of the environment and thus
generelize better to new unseen situations.

Given that well established separation in Reinforcement Learning there are different approaches that might yield improvements in abstract reasoning for reinforcement learning agents. The approach of Object-Centricity tries to
improve RL-Agents by not focusing on for example pixel representations of the environment (given that the environment might be some kind of game like the Atari Benchmark) but rather translating those pixel based representation
to abstract relational concepts (e.g. separating a game screen into distinct entities like player character, enemies, and collectibles rather than processing it as an undifferentiated grid of pixels).



Currently there are no attempts to combine model based approaches and Object-Centric ones. Since both have the goal of improving abstract reasoning by developing a more abstract understanding
 of the world and improving sample efficiency the vision is to use both things to arrive at a more robust solution.
A common pitfall of Reinforcement Learning is the misalignment problem, which can be summarized to an agent learning a policy, which indeed maximizes its reward but which does not represent
 what humans would like the agent to learn. This can for example be the agent exploiting some flaw in the environment and therefore avoiding to actually play and therefore learn the game.
  Moreover model free approaches are usually fairly sample inefficient since the agent has no other way of gaining examples (performing roll outs) then actually playing the game and exploring it.
Lastly generalization of the agents is often a big problems. Changing small and meaningless things in the environment sometimes lead to disastrous drops in performance. For example for Agents
 performing on pixel representations this might involve changing the background color. For humans this would not make a difference but agents sometimes fail, since they do no have a way to 
 actually understand the game they are playing but rather rely on certain tricks that they learn during training which are not robust to changes.

The proposed solution combines model-based reinforcement learning with object-centered representations to solve the identified problems. This integration offers several potential benefits.

First of all, model-based approaches can significantly improve sampling efficiency by allowing the agent to learn from simulated experiences generated by the world model. In combination with object-centered representations,
 the world model can make predictions in a more structured and compact state space that focuses on relevant objects and their relationships rather than raw pixels. This should enable more efficient learning with fewer environmental interactions.

Second, an object-centric world model could improve generalization capabilities. By decomposing scenes into objects and their relationships, the agent develops an understanding of the environment that is more robust to
 superficial changes. The hope is that this structured representation combined with model based approaches enhances generalization abilities even further.

Third, the combination could improve interpretability. Object-centered representations make it easier to understand what the agent considers important in the environment and how it reasons 
about state transitions. This interpretability could help identify and address mismatch issues by providing insight into why an agent makes certain decisions or why it might exploit loopholes in the environment.

The implementation strategy will focus on integrating object-centered perception modules into established model-based RL frameworks. This will require the development of methods to incorporate
 object-centered representations into world model predictions.

(TODO Write down some points here maybe merge into Introduction basically stating what the research currently says and what is missing -> what we want to do here)

\chapter{Background and Related Work}
\label{chap:background}

\section{Reinforcement Learning}
\label{sec:reinforcement_learning}

Reinforcement Learning (RL) is a machine learning paradigm that involves training agents to make sequential decisions by interacting with an environment. At its core, RL is often formalized using Markov Decision Processes (MDPs), where the environment's dynamics are defined by states, actions, transition probabilities, and reward functions. RL can be broadly categorized into model-free and model-based approaches.

In model-free RL, the agent learns a policy directly from interactions with the environment without constructing an explicit model of the world. By contrast, model-based RL incorporates a world model to predict future states or rewards, enabling more sample-efficient training by simulating rollouts in the constructed model. This abstraction allows agents to generalize better and potentially handle unseen situations more effectively.

\section{Dreamer Architecture}
\label{sec:dreamer_architecture}

The Dreamer architecture represents a family of model-based RL algorithms, namely DreamerV1, V2, and V3, that focus on combining world models with actor-critic methods for policy and value learning. Dreamer utilizes latent dynamics models to predict environment behavior in a compact latent space, enabling the agent to train policies on imagined rollouts. Notable predecessors like PlaNet introduced the concept of latent space dynamics, which Dreamer extends by incorporating actor-critic frameworks.

DreamerV2 builds on DreamerV1 by improving stability and scalability to discrete action spaces, commonly used in Atari benchmarks. DreamerV3 further generalizes the approach to handle diverse tasks with limited hyperparameter tuning.



Object-Centric Reinforcement Learning (OCRL) emphasizes representing an environment as a collection of interacting entities, rather than processing the entire environment as raw inputs, such as pixels. This paradigm improves sample efficiency, generalization, and interpretability by focusing on objects and their relationships.

Object-centric model-based RL combines the abstraction of object representations with predictive world models. By leveraging structured object-centric states, agents can reason about dynamic interactions in a more robust and interpretable manner compared to pixel-based representations.


There are multiple way to to train a world model. As mentioned above already you can predict latent states or directly predict future observations. There are also multiple different architectures which can do the job. From a simple MLP over LSTMs to Transformers there have been many different approiaches to that same problem. Tes

DreamerV2 \cite{hafner2019dreamer} is a model-based RL algorithm.

Explain in general what dreamer does
what are the components and how do they play together

The $\lambda$-target is defined recursively as:
\begin{equation}
	V^{\lambda}_t = \hat{r}_t + \hat{\gamma}_t \cdot \begin{cases}
		(1 - \lambda)v_\xi(\hat{z}_{t+1}) + \lambda V^{\lambda}_{t+1} & \text{if } t < H \\
		v_\xi(\hat{z}_H)                                              & \text{if } t = H
	\end{cases}
\end{equation}

where $\hat{r}_t$ represents the predicted reward, $\hat{\gamma}_t$ is the predicted discount factor, $v_\xi(\hat{z}_t)$ is the critic's value estimate, and $\lambda = 0.95$ controls the weighting between immediate and future rewards. This formulation creates a weighted average of n-step returns, where longer horizons receive exponentially decreasing weights.

In my case I dont have z but simply the observation itself. So no latent space.

The actor network in DreamerV2 employs a sophisticated loss function that combines multiple gradient estimators to achieve both learning efficiency and convergence stability. The actor aims to maximize the same $\lambda$-return targets used for critic training, incorporating intermediate rewards directly rather than relying solely on terminal value estimates.

The combined actor loss function is formulated as:
\begin{align}
	\mathcal{L}(\psi) = \mathbb{E}\left[\sum_{t=1}^{H-1} \Big[\right. & -\rho \ln p_\psi(\hat{a}_t | \hat{z}_t) \, \text{sg}(V^{\lambda}_t - v_\xi(\hat{z}_t)) \quad \text{[REINFORCE]} \\
	                                                                  & -(1-\rho)V^{\lambda}_t \quad \text{[Dynamics Backprop]}                                                         \\
	                                                                  & -\eta \mathcal{H}[a_t|\hat{z}_t] \quad \text{[Entropy Regularization]} \Big]
\end{align}

The loss function incorporates three distinct components:

\textbf{REINFORCE Gradients:}  REINFORCE algorithm, which maximizes the log-probability of actions weighted by their advantage values. The advantage is computed to be the difference between the lambda-return and the critic's estimate, with gradients stopped around the targets (denoted by sg stop gradient) to prevent interference with critic learning

\textbf{Entropy Regularization:} The third term encourages exploration by maximizing the entropy of the action distribution. The entropy coefficient $\eta$ controls the trade-off between exploitation and exploration, with higher values promoting more diverse action selection.

The weighting parameter $\rho$ determines the relative contribution of REINFORCE versus straight-through gradients. For discrete action spaces like Atari, DreamerV2 typically uses $\rho = 1$ (pure REINFORCE) with $\eta = 10^{-3}$, while continuous control tasks benefit from $\rho = 0$ (pure dynamics backpropagation) with $\eta = 10^{-4}$.

The critic network in DreamerV2 serves as a value function approximator that estimates the expected discounted sum of future rewards from any given latent state. This component is essential for both the $\lambda$-target computation and providing baseline estimates for the REINFORCE algorithm, making it a cornerstone of the learning process.

The critic is trained using temporal difference learning with the $\lambda$-targets as regression targets. The loss function is formulated as a squared error between the critic's predictions and the computed $\lambda$-returns:
\begin{equation}
	\mathcal{L}(\xi) = \mathbb{E}\left[\sum_{t=1}^{H-1} \frac{1}{2} \left(v_\xi(\hat{z}_t) - \text{sg}(V^{\lambda}_t)\right)^2\right]
\end{equation}

where $v_\xi(\hat{z}_t)$ represents the critic's value estimate for latent state $\hat{z}_t$, and $\text{sg}(V^{\lambda}_t)$ denotes the $\lambda$-target with stopped gradients. The gradient stopping prevents the critic's learning from interfering with the target computation, maintaining the stability of the temporal difference updates.

Several key design choices enhance the critic's learning efficiency:

\textbf{Target Network Stabilization:} Following the approach used in Deep Q-Networks, DreamerV2 employs a target network that provides stable targets for critic learning. The target network is a delayed copy of the critic parameters, updated every 100 gradient steps. This approach prevents the rapid changes in the critic from destabilizing the learning targets.

\textbf{Trajectory Weighting:} The loss terms are weighted by cumulative predicted discount factors to account for episode termination probabilities. This weighting ensures that states likely to lead to episode endings receive appropriate emphasis during training.

\textbf{Compact State Representation:} Unlike traditional value functions that operate on high-dimensional observations, the DreamerV2 critic leverages the compact latent states $\hat{z}_t$ learned by the world model. This representation provides several advantages: reduced computational complexity, better generalization across similar states, and improved learning efficiency due to the structured nature of the latent space.

The critic architecture consists of a multi-layer perceptron with ELU activations and approximately 1 million trainable parameters. The network outputs a single scalar value representing the expected return from the input state, enabling efficient batch processing of imagined trajectories during training.

% \subsection{TD Lambda Targets}
% \label{subsec:tdlambdatargets}

% \subsection{TD Lambda Targets}
% \label{subsec:tdlambdatargets}



\section{Object-Centric Reinforcement Learning}
\label{sec:object_centric_rl}
(TODO mention object-centric model-based RL)

\begin{itemize}
	\item \textbf{High Dimensionality:} 
	\item \textbf{Poor Generalization:} 
	\item \textbf{Lack of Semantic Understanding:} 
	\item \textbf{Sample Inefficiency:} 
	\item \textbf{Sensitivity to Noise and Distractors:} 
\end{itemize}



In contrast to pixel-based methods, object-centric learning approaches leverage understanding of objects to create an abstract representation of the environment. This mimics human perception,
 which naturally segments scenes into distinct entities and focuses on their interactions \cite{nanbo2021learningobjectcentricrepresentationsmultiobject}. Object-centric learning represents a paradigm shift in how reinforcement learning agents process and understand their environments.

Therefore object centricity provides several advantages over pixel based methods.

\textbf{Compositional Understanding:} Object-centric representations naturally excel at treating input features as compositions of distinct entities. Making them understand that certain features
correspond to the velocity and position of a ball and others to a paddle. This compositionality allows agents to reason about individual objects and their interactions.

\textbf{Improved Generalization:} By focusing on objects rather than pixel patterns, agents can generalize better across visually different but similar scenarios. For example an agent that
 understands the concept of a "ball" an an abstract obect can use this knowledge in environments where the ball has a different shape or appearance in general. (SOURCE)

\textbf{Enhanced Interpretability:} Object-centric representations provide naturally a better interpretability then pixel based representations since the agent can reason better that it perceives things as objects rather than "random" pixels.

\textbf{Sample Efficiency:} The structured nature of object-centric representations often leads to more sample-efficient learning. By working with compact, meaningful features rather than high-dimensional
 pixel data, agents can learn policies with fewer rollouts. THis is possible because the object space is already some kind of latent space which is usually the way things work (SOURCE)

A significant development in this field is the introduction of OCAtari (Object-Centric Atari) by Delfosse et al. \cite{delfosse2024ocatariobjectcentricatari2600}. OCAtari extends the widely-used Arcade
 Learning Environment (ALE) by providing resource-efficient extraction of object-centric states for Atari 2600 games. THis framework fixes a gap, where despite growing interest in object-centric approaches,
no standardized benchmark existed for evaluating such methods on the popular Atari domain.



The OCAtari framework works by two ways of extracting object states. It can either extract the object states directly from the emulator's RAM or by using template matching on the rendered frames. 
The RAM-based extraction is more efficient and accurate.
Directly having those extracted object states allows for significantly faster training times and also supports researches in making comparable findings since everyone can use the same object states.

The importance of object-centric approaches in Atari environments is esspecially present in light of pixel-based methods in these domains. Delfosse et al. demonstrated that deep 
RL agents without interpretable object-centric representations can learn misaligned policies even in simple games like Pong. \cite{delfosse2024interpretableconceptbottlenecksalign} 
This misalignment problem strengthens the argument for using object-centric understanding rather than attempting to retrofit interpretability onto pixel-based systems.

(TODO TALK ABOUT JAXATARI)


Object-centric representations naturally lend themselves to relational reasoning, where agents must understand not just individual objects but also
the relationships and interactions between them. This capability is crucial for complex decision-making in multi-object environments where the
 optimal policy depends on understanding how different entities influence each other.

Relational reasoning in reinforcement learning encompasses several key aspects:

\textbf{Spatial Relationships:} Understanding the relative positions of objects and how spatial configurations affect optimal actions. For example, in Pong, the relationship between the paddle position 
and ball trajectory determines the appropriate movement strategy.

\textbf{Temporal Relationships:} Tracking how object relationships evolve over time and predicting future interactions. This temporal aspect is particularly important for planning and anticipatory behavior.

\textbf{Causality:} (SOURCE) Recognizing cause-and-effect relationships between actions and object state changes. This understanding enables more sophisticated planning and can help avoid unintended consequences.

The integration of relational reasoning with model-based approaches offers significant potential for improving agent performance. By incorporating relational structure into world models, agents can make more 
accurate predictions about future states and plan more effectively. This integration forms a core component of our proposed approach, where object-centric world models explicitly encode relational information to enhance both prediction accuracy and policy learning.



\chapter{Methodology}
\label{chap:methodology}



\section{Object-Centric World Model and integration with Model based RL}
\label{sec:oc_world_model}




\section{World Model Architecture}
\label{sec:world_model_arch}


\subsection{LSTM-Based World Model (PongLSTM)}
\label{subsec:ponglstm}
explain LSTM
explain why is might be good for world models in general instead of models that only take the last state into account


\subsection{Alternative Architectures Explored}
\label{subsec:alternative_architectures}
Talk about MLP, Transformer, other RNNs


\subsection{State Normalization and Stability}
\label{subsec:normalization}
why normalization helps in RL in general and why it was used here

\section{Actor-Critic Integration}
\label{sec:actor_critic}
Explain how the actor-critic framework is integrated with the object-centric world model. 
Discuss any modifications made to the standard actor-critic approach to accommodate the 
object-centric representations.

\subsection{DreamerV2-Style Actor-Critic}
\label{subsec:dreamer_ac}
Explain the Dreamer approach and how it differs which is mainly in the latent space

\subsection{Policy Learning in Imagined Rollouts}
\label{subsec:imagined_rollouts}
how the policy is learned in imagined rollouts from the world model

\subsection{Lambda-Return Computation}
\label{subsec:lambda_returns}
provide the formular here and briefly explain it 

\section{Training Pipeline}
\label{sec:training_pipeline}
explain when the worldmodel gets retrained and how sampling works

\subsection{World Model Training and experience collection}
\label{subsec:world_model_training}
explain how the initial experience for the world model is collected and how it can be updated during later training stages

\subsection{Policy Optimization}
\label{subsec:policy_optimization}
explain how the policy is optimized and how often

\chapter{Experiments and Results}
\label{chap:experiments}




\section{Experimental Setup}
\label{sec:exp_setup}
* Experimental Design: Baseline, Comparisons, Eval Metrics
* Environment and Setup: Pong Environment Characteristics, Object-Centric State Description 
* Hyperparameters and Configuration: Model Architectures, Training Regimes
* Hardware and Implementation Details: Computational Resources, Software Frameworks
* Evaluation Protocol: Training and Testing Procedures, Statistical Analysis

\section{World Model Performance}
\label{sec:world_model_perf}

\subsection{Prediction Accuracy Analysis}
\label{subsec:prediction_accuracy}
Answer here : (RQ1) How does the prediction accuracy of object-centric world models compare to pixel-based world models in terms of state transition prediction and long-term rollout quality in the Pong environment?

\subsection{Long-Term Rollout Quality}
\label{subsec:rollout_quality}
Answer here : (RQ1) How does the prediction accuracy of object-centric world models compare to pixel-based world models in terms of state transition prediction and long-term rollout quality in the Pong environment?

\subsection{Model Stability Assessment}
\label{subsec:stability}
Answer here : (RQ1) How does the prediction accuracy of object-centric world models compare to pixel-based world models in terms of state transition prediction and long-term rollout quality in the Pong environment?

\section{Policy Learning Results}
\label{sec:policy_results}

\subsection{Sample Efficiency Comparison}
\label{subsec:sample_efficiency_comp}
Answer here : (RQ1) To what extent does integrating object-centric representations with model-based RL improve sample efficiency compared to model-free baselines and pixel-based model-based approaches?


\subsection{Final Performance Evaluation}
\label{subsec:final_performance}
Answer here : (RQ1) To what extent does integrating object-centric representations with model-based RL improve sample efficiency compared to model-free baselines and pixel-based model-based approaches?


\subsection{Learning Curve Analysis}
\label{subsec:learning_curves}
Answer here : (RQ1) To what extent does integrating object-centric representations with model-based RL improve sample efficiency compared to model-free baselines and pixel-based model-based approaches?


\section{Ablation Studies}
\label{sec:ablation_studies}

\subsection{Impact of Object-Centric Representations}
\label{subsec:oc_impact}
Answer here : (RQ2) How effectively can the DreamerV2-style actor-critic framework learn optimal policies when trained on imagined rollouts from object-centric world models?


\subsection{Architecture Component Analysis}
\label{subsec:architecture_analysis}
Answer here : (RQ3) What is the impact of different world model architectures (LSTM-based vs. alternatives) on the overall performance of object-centric model-based RL in structured environments?


\subsection{Reward Function Design Effects}
\label{subsec:reward_effects}
Answer here : (RQ3) What is the impact of different world model architectures (LSTM-based vs. alternatives) on the overall performance of object-centric model-based RL in structured environments?


\subsection{Real vs. Model Comparison}
\label{subsec:real_vs_model}
Answer here : (RQ2) How effectively can the DreamerV2-style actor-critic framework learn optimal policies when trained on imagined rollouts from object-centric world models?


\subsection{Policy Behavior Visualization}
\label{subsec:policy_visualization}
Answer here : (RQ2) How effectively can the DreamerV2-style actor-critic framework learn optimal policies when trained on imagined rollouts from object-centric world models?

\chapter{Discussion}
\label{chap:discussion}
* Challenges during training, what decreases model performance

* Analysis of Results
	- Strengths of the Proposed Approach
	- Limitations and Challenges
	- Comparison with Existing Methods

* Technical Insights
	- World Model Design Choices
	- Training Stability Issues
	- Reward Engineering Importance

* Implications for Object-Centric RL
	- Benefits of Integration
	- Generalization Potential
	- Scalability Considerations

* Future Research Directions
	- More Complex Environments
	- Improved Object Discovery
	- Multi-Object Scenarios

\chapter{Conclusion and Future Work}
\label{chap:conclusion}

\begin{itemize}
	\item Summary of Contributions
	\item Key Findings
	\item Limitations and Future Work
	\item Final Remarks
\end{itemize}

\printbibliography[title={References}]

\appendix

\chapter{Implementation Details}
\label{app:implementation}

\section{Code Structure and Organization}
\label{app:code_structure}
Provide an overview of the project's codebase, including the main modules, their purposes, and how they interact.

\section{Hyperparameter Sensitivity Analysis}
\label{app:hyperparameter_analysis}
Discuss the impact of varying key hyperparameters on model performance and training stability.

\section{Additional Experimental Results}
\label{app:additional_results}
Include supplementary experimental results that support the main findings, such as extended comparisons or detailed metrics.

\chapter{Technical Specifications}
\label{app:technical_specs}

\section{Hardware Requirements}
\label{app:hardware_requirements}
List the hardware specifications required to reproduce the experiments, including GPU/CPU details and memory requirements.

\section{Software Dependencies}
\label{app:software_dependencies}
Detail the software libraries, frameworks, and versions used in the implementation.

\section{Reproducibility Guidelines}
\label{app:reproducibility}
Provide step-by-step instructions for reproducing the experiments, including setup, data preparation, and execution.

\chapter{Supplementary Figures and Tables}
Include additional figures and tables that complement the main text, such as visualizations of training curves or ablation study results.
\label{app:supplementary}

\end{document}