%% Master Thesis Table of Contents
%% Integrating Object-Centric Learning with Model-Based Reinforcement Learning

\documentclass[
	english,
	ruledheaders=section,
	class=report,
	thesis={type=master},
	accentcolor=9c,
	custommargins=true,
	marginpar=false,
	parskip=half-,
	fontsize=11pt,
]{tudapub}



\usepackage{iftex}
\ifPDFTeX
	\usepackage[utf8]{inputenc}
\fi



%%%%%%%%%%%%%%%%%%%
% Language settings
%%%%%%%%%%%%%%%%%%%
\usepackage{babel}
\usepackage[autostyle]{csquotes}
\usepackage{microtype}


%%%%%%%%%%%%%%%%%%%
% Bibliography
%%%%%%%%%%%%%%%%%%%
\usepackage{biblatex}
\addbibresource{references.bib}


\DefineBibliographyStrings{english}{
  bibliography = {References},
}


%%%%%%%%%%%%%%%%%%%
% Table packages
%%%%%%%%%%%%%%%%%%%
\usepackage{tabularx}
\usepackage{booktabs}

%%%%%%%%%%%%%%%%%%%
% Math packages
%%%%%%%%%%%%%%%%%%%
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{siunitx}

%%%%%%%%%%%%%%%%%%%
% Additional packages
%%%%%%%%%%%%%%%%%%%
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subcaption}

\begin{document}

\Metadata{
	title=Structured World Understanding: Integrating Object-Centric Learning with Model-Based Reinforcement Learning,
	author=Your Name
}

\title{Structured World Understanding: Integrating Object-Centric Learning with Model-Based Reinforcement Learning}
\subtitle{Master's Thesis: Summer Semester 2025}
\author[Y. Name]{Your Full Name}
\studentID{1234567}
\reviewer{Jannis Bl√ºml}

\department{Department of Computer Science}
\institute{Institute of Computer Science}
\group{Machine Learning Research Group}

\submissiondate{24.10.2025}
\examdate{24.10.2025}

\maketitle

\affidavit

\tableofcontents
\listoffigures
\listoftables

% List of Abbreviations
\chapter*{List of Abbreviations}
\addcontentsline{toc}{chapter}{List of Abbreviations}
\begin{tabular}{ll}
	RL   & Reinforcement Learning              \\
	MBRL & Model-Based Reinforcement Learning  \\
	MFRL & Model-Free Reinforcement Learning   \\
	CNN  & Convolutional Neural Network        \\
	LSTM & Long Short-Term Memory              \\
	PPO  & Proximal Policy Optimization        \\
	A3C  & Asynchronous Advantage Actor-Critic \\
	DQN  & Deep Q-Network                      \\
	RSSM & Recurrent State Space Model         \\
	KL   & Kullback-Leibler                    \\
	MSE  & Mean Squared Error                  \\
	RGB  & Red-Green-Blue                      \\
	GPU  & Graphics Processing Unit            \\
	JAX  & Just After eXecution                \\
\end{tabular}

\chapter{Introduction}
\label{chap:introduction}

\section{Context}
\label{sec:context}

Reinforcement Learning as one of the three categories in Machine Learning among Supervised and Unsupervised Learning follows the approach of having an agent performs actions in an environment and receiving rewards for certain actions. So unlike Supervised Learning for example there is no ground truth from the which the agent can learn from. Among numerous distinctions within Reinforcement Learning there is the difference between Model Based Reinforcement Learning and Model Free Reinforcement Learning. The Latter focuses on the learning of only the Agent which is deployed in the environment (or learns from samples). Thus the agent will have no explicit way of predicting the next state of the environment, although sufficiently large for example neural networks might create some implicit model of the world. Model based Reinforcement learning augments the architecture by a Model that is predicting the next state of the world but also still trains an agent. So there are usually two separate models. That might improve sample efficiency since once the world model is trained it can be used to train the agent without any need of additional sampling in the environment. Another hope of Model based approaches is that they can achiever a better understanding of the environment and thus generelize better to new unseen situations.

Given that well established separation in Reinforcement Learning there are different approaches that might yield improvements in abstract reasoning for reinforcement learning agents. The approach of Object-Centricity tries to improve RL-Agents by not focusing on for example pixel representations of the environment (given that the environment might be some kind of game like the Atari Benchmark) but rather translating those pixel based representation to abstract relational concepts (e.g. separating a game screen into distinct entities like player character, enemies, and collectibles rather than processing it as an undifferentiated grid of pixels).

\section{Goal}
\label{sec:goal}

\textbf{The task of this Thesis can be summarized as follows:}
Currently there are no attempts to combine model based approaches and Object-Centric ones. Since both have the goal of improving abstract reasoning by developing a more abstract understanding of the world and improving sample efficiency the vision is to use both things to arrive at a more robust solution.

\subsection{Problem}
\label{subsec:problem}

A common pitfall of Reinforcement Learning is the misalignment problem, which can be summarized to an agent learning a policy, which indeed maximizes its reward but which does not represent what humans would like the agent to learn. This can for example be the agent exploiting some flaw in the environment and therefore avoiding to actually play and therefore learn the game. Moreover model free approaches are usually fairly sample inefficient since the agent has no other way of gaining examples (performing roll outs) then actually playing the game and exploring it.

Lastly generalization of the agents is often a big problems. Changing small and meaningless things in the environment sometimes lead to disastrous drops in performance. For example for Agents performing on pixel representations this might involve changing the background color. For humans this would not make a difference but agents sometimes fail, since they do no have a way to actually understand the game they are playing but rather rely on certain tricks that they learn during training which are not robust to changes.

\subsection{Solution}
\label{subsec:solution}

The proposed solution combines model-based reinforcement learning with object-centered representations to solve the identified problems. This integration offers several potential benefits.

First of all, model-based approaches can significantly improve sampling efficiency by allowing the agent to learn from simulated experiences generated by the world model. In combination with object-centered representations, the world model can make predictions in a more structured and compact state space that focuses on relevant objects and their relationships rather than raw pixels. This should enable more efficient learning with fewer environmental interactions.

Second, an object-centric world model could improve generalization capabilities. By decomposing scenes into objects and their relationships, the agent develops an understanding of the environment that is more robust to superficial changes. The hope is that this structured representation combined with model based approaches enhances generalization abilities even further.

Third, the combination could improve interpretability. Object-centered representations make it easier to understand what the agent considers important in the environment and how it reasons about state transitions. This interpretability could help identify and address mismatch issues by providing insight into why an agent makes certain decisions or why it might exploit loopholes in the environment.

The implementation strategy will focus on integrating object-centered perception modules into established model-based RL frameworks. This will require the development of methods to incorporate object-centered representations into world model predictions.

\section{Thesis Structure}
\label{sec:structure}

This thesis is organized as follows: Chapter~\ref{chap:background} provides the necessary background on reinforcement learning and object-centric representations. Chapter~\ref{chap:methodology} presents the theoretical framework and integration strategy. Chapter~\ref{chap:implementation} details the technical implementation. Chapter~\ref{chap:experiments} presents experimental results and analysis. Chapter~\ref{chap:discussion} discusses the implications and limitations of the approach. Chapter~\ref{chap:conclusion} concludes the thesis and outlines future research directions.

\chapter{Background and Related Work}
\label{chap:background}

\section{Reinforcement Learning Fundamentals}
\label{sec:rl_fundamentals}

\subsection{Model-Free vs. Model-Based Approaches}
\label{subsec:mf_vs_mb}
There are two main categories in Reinforcement learning. First of all Model-Free Reinforcement Learning focousses on training an agent without explicitly learning a model of the environment. There is no prediction of future states but purely an agent that takes in observation and directly computes Decisions. Whereas Model Based Reinforcement Learning usually consists of a world model part that tries to make predicitons on how the environment will behave in the future. This can be done using actual predicition of next observations directly or using some kind of latent space and making predictions there.

\subsection{Sample Efficiency Challenges}
\label{subsec:sample_efficiency}
Sample Efficiency so the optimisation of the amount of rollouts that is sampled in the actual environment can have multiple motivations. On the one hand the actual environment can be something in the physical world where there are contraints on the amount of times a physical object like a robot can move or how fast one can generate real samples. On the other hand computing states in the actual environment might me more costly and can therefore lower training speed depending on the complexity of the environment. For example running a game can be more resource intensive than computing a next (latent) state in the internal representation.
Model based approaches usually vastly improve sample effiency because the data hungry step of training the agent can be done in the worldmodel. Meaning the worldmodel creates artificial rollouts (or parts of them) in which the actor can then do its training. The worldmodel has to of course sample training data in the actual environment but it usually requires way less training steps.

\subsection{Generalization Problems}
\label{subsec:generalization}
Moreover Model-Free approaches  sometimes suffer from generalization problems. They overfit on a certain state of the environment. A common thing is the background color of the game if the actor is training on visual input. Thus changing the background which would have zero impact on the performance of a human player can drastically decrease the performance of an agent. Model Based Approaches usually come with some form of abstraction be it latent space or object centric things which usually do not suffer that much from simple changes in the environment. Also due to the error of the Environment Prediction an actor might see more diverse things to begin with.

\section{Model-Based Reinforcement Learning}
\label{sec:mbrl}

\subsection{World Model Learning}
\label{subsec:world_models}
There are multiple way to to train a world model. As mentioned above already you can predict latent states or directly predict future observations. There are also multiple different architectures which can do the job. From a simple MLP over LSTMs to Transformers there have been many different approiaches to that same problem. Tes

\subsection{Planning with Learned Models}
\label{subsec:planning}

\subsection{DreamerV2 Architecture}
\label{subsec:dreamerv2_arch}
DreamerV2 \cite{hafner2019dreamer} is a model-based RL algorithm.

Explain in general what dreamer does
what are the components and how do they play together

\subsection{DreamerV2 TD Lambda Targets}
\label{subsec:td_lambda}

The $\lambda$-target is defined recursively as:
\begin{equation}
	V^{\lambda}_t = \hat{r}_t + \hat{\gamma}_t \cdot \begin{cases}
		(1 - \lambda)v_\xi(\hat{z}_{t+1}) + \lambda V^{\lambda}_{t+1} & \text{if } t < H \\
		v_\xi(\hat{z}_H)                                              & \text{if } t = H
	\end{cases}
\end{equation}

where $\hat{r}_t$ represents the predicted reward, $\hat{\gamma}_t$ is the predicted discount factor, $v_\xi(\hat{z}_t)$ is the critic's value estimate, and $\lambda = 0.95$ controls the weighting between immediate and future rewards. This formulation creates a weighted average of n-step returns, where longer horizons receive exponentially decreasing weights.

In my case I dont have z but simply the observation itself. So no latent space.

\subsection{DreamerV2 Actor Loss}
\label{subsec:actor_loss}

The actor network in DreamerV2 employs a sophisticated loss function that combines multiple gradient estimators to achieve both learning efficiency and convergence stability. The actor aims to maximize the same $\lambda$-return targets used for critic training, incorporating intermediate rewards directly rather than relying solely on terminal value estimates.

The combined actor loss function is formulated as:
\begin{align}
	\mathcal{L}(\psi) = \mathbb{E}\left[\sum_{t=1}^{H-1} \Big[\right. & -\rho \ln p_\psi(\hat{a}_t | \hat{z}_t) \, \text{sg}(V^{\lambda}_t - v_\xi(\hat{z}_t)) \quad \text{[REINFORCE]} \\
	                                                                  & -(1-\rho)V^{\lambda}_t \quad \text{[Dynamics Backprop]}                                                         \\
	                                                                  & -\eta \mathcal{H}[a_t|\hat{z}_t] \quad \text{[Entropy Regularization]} \Big]
\end{align}

The loss function incorporates three distinct components:

\textbf{REINFORCE Gradients:}  REINFORCE algorithm, which maximizes the log-probability of actions weighted by their advantage values. The advantage is computed to be the difference between the lambda-return and the critic's estimate, with gradients stopped around the targets (denoted by sg stop gradient) to prevent interference with critic learning

\textbf{Entropy Regularization:} The third term encourages exploration by maximizing the entropy of the action distribution. The entropy coefficient $\eta$ controls the trade-off between exploitation and exploration, with higher values promoting more diverse action selection.

The weighting parameter $\rho$ determines the relative contribution of REINFORCE versus straight-through gradients. For discrete action spaces like Atari, DreamerV2 typically uses $\rho = 1$ (pure REINFORCE) with $\eta = 10^{-3}$, while continuous control tasks benefit from $\rho = 0$ (pure dynamics backpropagation) with $\eta = 10^{-4}$.

\subsection{DreamerV2 Critic Loss}
\label{subsec:critic_loss}

The critic network in DreamerV2 serves as a value function approximator that estimates the expected discounted sum of future rewards from any given latent state. This component is essential for both the $\lambda$-target computation and providing baseline estimates for the REINFORCE algorithm, making it a cornerstone of the learning process.

The critic is trained using temporal difference learning with the $\lambda$-targets as regression targets. The loss function is formulated as a squared error between the critic's predictions and the computed $\lambda$-returns:
\begin{equation}
	\mathcal{L}(\xi) = \mathbb{E}\left[\sum_{t=1}^{H-1} \frac{1}{2} \left(v_\xi(\hat{z}_t) - \text{sg}(V^{\lambda}_t)\right)^2\right]
\end{equation}

where $v_\xi(\hat{z}_t)$ represents the critic's value estimate for latent state $\hat{z}_t$, and $\text{sg}(V^{\lambda}_t)$ denotes the $\lambda$-target with stopped gradients. The gradient stopping prevents the critic's learning from interfering with the target computation, maintaining the stability of the temporal difference updates.

Several key design choices enhance the critic's learning efficiency:

\textbf{Target Network Stabilization:} Following the approach used in Deep Q-Networks, DreamerV2 employs a target network that provides stable targets for critic learning. The target network is a delayed copy of the critic parameters, updated every 100 gradient steps. This approach prevents the rapid changes in the critic from destabilizing the learning targets.

\textbf{Trajectory Weighting:} The loss terms are weighted by cumulative predicted discount factors to account for episode termination probabilities. This weighting ensures that states likely to lead to episode endings receive appropriate emphasis during training.

\textbf{Compact State Representation:} Unlike traditional value functions that operate on high-dimensional observations, the DreamerV2 critic leverages the compact latent states $\hat{z}_t$ learned by the world model. This representation provides several advantages: reduced computational complexity, better generalization across similar states, and improved learning efficiency due to the structured nature of the latent space.

The critic architecture consists of a multi-layer perceptron with ELU activations and approximately 1 million trainable parameters. The network outputs a single scalar value representing the expected return from the input state, enabling efficient batch processing of imagined trajectories during training.

% \subsection{TD Lambda Targets}
% \label{subsec:tdlambdatargets}

% \subsection{TD Lambda Targets}
% \label{subsec:tdlambdatargets}

\section{Object-Centric Representations}
\label{sec:object_centric}

\subsection{Limitations of Pixel-Based Representations}
\label{subsec:pixel_limitations}


\begin{itemize}
	\item \textbf{High Dimensionality:} Pixel observations are inherently high-dimensional, leading to increased computational requirements and slower learning. The agent must process large amounts of redundant information, much of which may be irrelevant to the task.
	\item \textbf{Poor Generalization:} Agents trained on pixel data tend to overfit to specific visual details, such as background colors or textures. Even minor changes in the environment's appearance can significantly degrade performance, as the agent fails to recognize semantically identical states that look different at the pixel level.
	\item \textbf{Lack of Semantic Understanding:} Pixel-based representations do not explicitly encode objects or their relationships. As a result, agents struggle to reason about entities, interactions, or causal structure in the environment, limiting their ability to plan or transfer knowledge.
	\item \textbf{Sample Inefficiency:} Learning meaningful features directly from pixels typically requires a vast number of environment interactions. The agent must discover relevant abstractions from scratch, which is especially challenging in sparse-reward or complex environments.
	\item \textbf{Sensitivity to Noise and Distractors:} Pixel-based agents are vulnerable to visual noise, occlusions, or irrelevant objects. Such distractions can mislead the agent or disrupt the learning process, as there is no mechanism to focus on task-relevant entities.
\end{itemize}

These limitations motivate the exploration of object-centric representations, which aim to provide more structured, compact, and semantically meaningful state descriptions for reinforcement learning agents.

\subsection{Object-Centric Learning Approaches}
\label{subsec:oc_approaches}

In contrast to pixel-based methods, object-centric learning approaches leverage understanding of objects to create an abstract representation of the environment. This mimics human perception,
 which naturally segments scenes into distinct entities and focuses on their interactions \cite{nanbo2021learningobjectcentricrepresentationsmultiobject}. Object-centric learning represents a paradigm shift in how reinforcement learning agents process and understand their environments.

Therefore object centricity provides several advantages over pixel based methods.

\textbf{Compositional Understanding:} Object-centric representations naturally excel at treating input features as compositions of distinct entities. Making them understand that certain features
correspond to the velocity and position of a ball and others to a paddle. This compositionality allows agents to reason about individual objects and their interactions.

\textbf{Improved Generalization:} By focusing on objects rather than pixel patterns, agents can generalize better across visually different but similar scenarios. For example an agent that
 understands the concept of a "ball" an an abstract obect can use this knowledge in environments where the ball has a different shape or appearance in general. (SOURCE)

\textbf{Enhanced Interpretability:} Object-centric representations provide naturally a better interpretability then pixel based representations since the agent can reason better that it perceives things as objects rather than "random" pixels.

\textbf{Sample Efficiency:} The structured nature of object-centric representations often leads to more sample-efficient learning. By working with compact, meaningful features rather than high-dimensional
 pixel data, agents can learn policies with fewer rollouts. THis is possible because the object space is already some kind of latent space which is usually the way things work (SOURCE)

A significant development in this field is the introduction of OCAtari (Object-Centric Atari) by Delfosse et al. \cite{delfosse2024ocatariobjectcentricatari2600}. OCAtari extends the widely-used Arcade
 Learning Environment (ALE) by providing resource-efficient extraction of object-centric states for Atari 2600 games. THis framework fixes a gap, where despite growing interest in object-centric approaches,
no standardized benchmark existed for evaluating such methods on the popular Atari domain.

(UNTIL HERE)

The OCAtari framework operates by maintaining object-centric states during game execution, providing a list of depicted objects and their properties for each game frame. This approach eliminates the need 
for separate object discovery methods during training, significantly improving computational efficiency. The framework supports various research directions including object discovery, object representation learning, and object-centric reinforcement learning.

The importance of object-centric approaches in Atari environments is particularly evident when considering the limitations of pixel-based methods in these domains. Delfosse et al. demonstrated that deep 
RL agents without interpretable object-centric representations can learn misaligned policies even in simple games like Pong, highlighting fundamental issues that post-hoc explanation techniques fail to detect. 
This misalignment problem underscores the necessity of incorporating object-centric understanding from the ground up rather than attempting to retrofit interpretability onto pixel-based systems.

Furthermore, object-centric representations enable more sophisticated reasoning capabilities that mirror human cognitive processes. As Lake et al. illustrated with the "Frostbite challenge," deep agents 
trained on traditional ALE environments often lack the ability to create multi-step sub-goals, such as acquiring certain objects while avoiding others. Object-centric approaches provide the structured foundation necessary for such hierarchical reasoning.

The integration of object-centric learning with model-based reinforcement learning presents particularly promising opportunities. While traditional model-based approaches often struggle with the high-dimensional 
nature of pixel observations, object-centric representations provide a more tractable state space for world model learning. This combination can potentially achieve the sample efficiency benefits of model-based methods while maintaining the interpretability and generalization advantages of object-centric representations.

\subsection{Relational Reasoning in RL}
\label{subsec:relational_reasoning}

Object-centric representations naturally lend themselves to relational reasoning, where agents must understand not just individual objects but also the relationships and interactions between them. This capability is crucial for complex decision-making in multi-object environments where the optimal policy depends on understanding how different entities influence each other.

Relational reasoning in reinforcement learning encompasses several key aspects:

\textbf{Spatial Relationships:} Understanding the relative positions of objects and how spatial configurations affect optimal actions. For example, in Pong, the relationship between the paddle position and ball trajectory determines the appropriate movement strategy.

\textbf{Temporal Dependencies:} Tracking how object relationships evolve over time and predicting future interactions. This temporal aspect is particularly important for planning and anticipatory behavior.

\textbf{Causal Understanding:} Recognizing cause-and-effect relationships between actions and object state changes. This understanding enables more sophisticated planning and can help avoid unintended consequences.

\textbf{Hierarchical Abstractions:} Building higher-level abstractions that group objects or relationships into meaningful categories, enabling more efficient reasoning about complex scenarios.

The integration of relational reasoning with model-based approaches offers significant potential for improving agent performance. By incorporating relational structure into world models, agents can make more accurate predictions about future states and plan more effectively. This integration forms a core component of our proposed approach, where object-centric world models explicitly encode relational information to enhance both prediction accuracy and policy learning.

\section{Current Limitations and Research Gaps}
\label{sec:limitations}



\chapter{Methodology}
\label{chap:methodology}

\section{Theoretical Framework}
\label{sec:framework}

\subsection{Object-Centric World Model Design}
\label{subsec:oc_world_model}

\subsection{Integration Strategy with Model-Based RL}
\label{subsec:integration_strategy}

\subsection{Expected Benefits Analysis}
\label{subsec:benefits_analysis}

\section{Implementation Approach}
\label{sec:implementation_approach}

\subsection{Environment Selection (Atari Pong)}
\label{subsec:env_selection}

\subsection{Object-Centric State Representation}
\label{subsec:state_representation}

\subsection{World Model Architecture Design}
\label{subsec:architecture_design}

\section{Experimental Design}
\label{sec:experimental_design}

\subsection{Baseline Comparisons}
\label{subsec:baselines}

\subsection{Evaluation Metrics}
\label{subsec:metrics}

\chapter{Implementation}
\label{chap:implementation}

\section{Environment and Setup}
\label{sec:environment}

\subsection{Pong Environment Characteristics}
\label{subsec:pong_characteristics}

\subsection{Object-Centric State Description}
\label{subsec:state_description}

\section{World Model Architecture}
\label{sec:world_model_arch}

\subsection{LSTM-Based World Model (PongLSTM)}
\label{subsec:ponglstm}

\subsection{Alternative Architectures Explored}
\label{subsec:alternative_architectures}

\subsection{State Normalization and Stability}
\label{subsec:normalization}

\section{Actor-Critic Integration}
\label{sec:actor_critic}

\subsection{DreamerV2-Style Actor-Critic}
\label{subsec:dreamer_ac}

\subsection{Policy Learning in Imagined Rollouts}
\label{subsec:imagined_rollouts}

\subsection{Lambda-Return Computation}
\label{subsec:lambda_returns}

\section{Training Pipeline}
\label{sec:training_pipeline}

\subsection{Experience Collection}
\label{subsec:experience_collection}

\subsection{World Model Training}
\label{subsec:world_model_training}

\subsection{Policy Optimization}
\label{subsec:policy_optimization}

\chapter{Experiments and Results}
\label{chap:experiments}

\section{Experimental Setup}
\label{sec:exp_setup}

\subsection{Hyperparameters and Configuration}
\label{subsec:hyperparameters}

\subsection{Hardware and Implementation Details}
\label{subsec:hardware}

\subsection{Evaluation Protocol}
\label{subsec:eval_protocol}

\section{World Model Performance}
\label{sec:world_model_perf}

\subsection{Prediction Accuracy Analysis}
\label{subsec:prediction_accuracy}

\subsection{Long-Term Rollout Quality}
\label{subsec:rollout_quality}

\subsection{Model Stability Assessment}
\label{subsec:stability}

\section{Policy Learning Results}
\label{sec:policy_results}

\subsection{Sample Efficiency Comparison}
\label{subsec:sample_efficiency_comp}

\subsection{Final Performance Evaluation}
\label{subsec:final_performance}

\subsection{Learning Curve Analysis}
\label{subsec:learning_curves}

\section{Ablation Studies}
\label{sec:ablation_studies}

\subsection{Impact of Object-Centric Representations}
\label{subsec:oc_impact}

\subsection{Architecture Component Analysis}
\label{subsec:architecture_analysis}

\subsection{Reward Function Design Effects}
\label{subsec:reward_effects}

\section{Visualization and Interpretability}
\label{sec:visualization}

\subsection{Real vs. Model Comparison}
\label{subsec:real_vs_model}

\subsection{Policy Behavior Visualization}
\label{subsec:policy_visualization}

\chapter{Discussion}
\label{chap:discussion}

\section{Analysis of Results}
\label{sec:results_analysis}

\subsection{Strengths of the Proposed Approach}
\label{subsec:strengths}

\subsection{Limitations and Challenges}
\label{subsec:limitations_challenges}

\subsection{Comparison with Existing Methods}
\label{subsec:comparison}

\section{Technical Insights}
\label{sec:technical_insights}

\subsection{World Model Design Choices}
\label{subsec:design_choices}

\subsection{Training Stability Issues}
\label{subsec:stability_issues}

\subsection{Reward Engineering Importance}
\label{subsec:reward_engineering}

\section{Implications for Object-Centric RL}
\label{sec:implications}

\subsection{Benefits of Integration}
\label{subsec:integration_benefits}

\subsection{Generalization Potential}
\label{subsec:generalization_potential}

\subsection{Scalability Considerations}
\label{subsec:scalability}

\section{Future Research Directions}
\label{sec:future_work}

\subsection{More Complex Environments}
\label{subsec:complex_environments}

\subsection{Improved Object Discovery}
\label{subsec:object_discovery}

\subsection{Multi-Object Scenarios}
\label{subsec:multi_object}

\chapter{Conclusion}
\label{chap:conclusion}

\section{Summary of Contributions}
\label{sec:summary_contributions}

\section{Key Findings}
\label{sec:key_findings}

\section{Limitations and Future Work}
\label{sec:limitations_future}

\section{Final Remarks}
\label{sec:final_remarks}

\printbibliography[title={References}]

\appendix

\chapter{Implementation Details}
\label{app:implementation}

\section{Code Structure and Organization}
\label{app:code_structure}

\section{Hyperparameter Sensitivity Analysis}
\label{app:hyperparameter_analysis}

\section{Additional Experimental Results}
\label{app:additional_results}

\chapter{Technical Specifications}
\label{app:technical_specs}

\section{Hardware Requirements}
\label{app:hardware_requirements}

\section{Software Dependencies}
\label{app:software_dependencies}

\section{Reproducibility Guidelines}
\label{app:reproducibility}

\chapter{Supplementary Figures and Tables}
\label{app:supplementary}

\end{document}