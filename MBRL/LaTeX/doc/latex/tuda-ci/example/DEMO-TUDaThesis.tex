%% Master Thesis Table of Contents
%% Integrating Object-Centric Learning with Model Based Reinforcement Learning

\documentclass[
	english,
	ruledheaders=section,
	class=report,
	thesis={type=master},
	accentcolor=9c,
	custommargins=true,
	marginpar=false,
	parskip=half-,
	fontsize=11pt,
]{tudapub}



\usepackage{iftex}
\ifPDFTeX
	\usepackage[utf8]{inputenc}
\fi



%%%%%%%%%%%%%%%%%%%
% Language settings
%%%%%%%%%%%%%%%%%%%
\usepackage[english]{babel}
\selectlanguage{english}
\usepackage[autostyle]{csquotes}
\usepackage{microtype}


%%%%%%%%%%%%%%%%%%%
% Bibliography
%%%%%%%%%%%%%%%%%%%
\usepackage[style=numeric,sorting=none,backend=biber]{biblatex}
\addbibresource{references.bib}


\DefineBibliographyStrings{english}{
  bibliography = {References},
}





%%%%%%%%%%%%%%%%%%%
% Table packages
%%%%%%%%%%%%%%%%%%%
\usepackage{tabularx}
\usepackage{booktabs}

%%%%%%%%%%%%%%%%%%%
% Math packages
%%%%%%%%%%%%%%%%%%%
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{siunitx}

%%%%%%%%%%%%%%%%%%%
% Additional packages
%%%%%%%%%%%%%%%%%%%
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subcaption}

\begin{document}

\Metadata{
	title=Structured World Understanding: Integrating Object-Centric Learning with Model Based Reinforcement Learning,
	author=Your Name
}

\title{Structured World Understanding: Integrating Object-Centric Learning with Model Based Reinforcement Learning}
\subtitle{Master's Thesis: Summer Semester 2025}
\author[Y. Name]{Your Full Name}
\studentID{1234567}
\reviewer{Prof. Dr. Kristian Kersting\\[0.2em]2. Review: Jannis Blüml, Raban Emunds}

\department{Department of Computer Science}
\group{Machine Learning Research Group (AI \& ML Lab)}

\submissiondate{24.10.2025}
\examdate{24.10.2025}

\maketitle

\affidavit

\tableofcontents
\listoffigures
\listoftables

% List of Abbreviations
\chapter*{List of Abbreviations}
\addcontentsline{toc}{chapter}{List of Abbreviations}
\begin{tabular}{ll}
	RL   & Reinforcement Learning              \\
	MBRL & Model Based Reinforcement Learning  \\
	MFRL & Model Free Reinforcement Learning   \\
	CNN  & Convolutional Neural Network        \\
	LSTM & Long Short-Term Memory              \\
	PPO  & Proximal Policy Optimization        \\
	A3C  & Asynchronous Advantage Actor-Critic \\
	DQN  & Deep Q-Network                      \\
	RSSM & Recurrent State Space Model         \\
	KL   & Kullback-Leibler                    \\
	MSE  & Mean Squared Error                  \\
	RGB  & Red-Green-Blue                      \\
	GPU  & Graphics Processing Unit            \\
	JAX  & Just After eXecution                \\
\end{tabular}



\chapter{Introduction}
\label{chap:introduction}

Modern Artificial Intelligence Approchaes often struggle with understanding and reasoning about complex 
environments in a structured, interpretable way. Reinforcement Learning in particular
suffers from agents learning control tasks or games especially well without grasping the underlying rules pf the game.
Unlike Supervised Learning for example there is no ground truth from the
which the agent can learn from. Among numerous distinctions within
reinforcement rearning there is the difference between Model Based
Reinforcement Learning and Model Free Reinforcement Learning. The latter
focuses on the learning of only the agent which is deployed in the environment
(or learns from samples). Thus the agent will have no explicit way of
predicting the next state of the environment. Model
based Reinforcement learning augments the architecture by a model that is
predicting the next state of the world but also still trains an agent. Although
model free approaches usually are less complex since they only have to learn
one model, the agent they also have significant disadvantages.

Model free learning often struggle with sample efficiency and
generalization to new tasks. Small changes in the environment can lead to
significant drops in performance. Moreover, agents can learn misaligned
policies that exploit loopholes in the environment rather than achieving the
intended goals. These challenges highlight the need for more structured and
interpretable learning methods. 

A solution to the lack of structure and interpretability can be Object Centricity, which focuses on learning or providing some kind of object representation that can be used for reasoning and decision-making. 
This can involve identifying and manipulating objects within the environment, leading to more interpretable and robust policies.
Built on top of Atari  OCAtari \cite{delfosse2023ocatari} provides a standardized framework for extracting object representations
from the standard Atari environment. This makes different object centric appraoches more comparable.

Object Centricity has the goal of 
interpretability and Robustness whereas Model Based approaches try to improve
sample efficiency and planning capabilities among others. This
thesis aims to investigate the potential benefits of integrating object-centric
learning into model based reinforcement learning frameworks to address these
challenges by harvesting the strengths of both paradigms. This thesis is
therefore guided by the following research questions:
\begin{enumerate}
	\item How do object-centric world models compare to pixel-based world models
	      regarding prediction accuracy and sample efficiency?
	\item Can an actor-critic framework be effectively trained on simulated rollouts from
	      object-centric world models?
	\item What is the impact of different world model architectures (such as LSTM-based
	      versus alternative designs) on the overall performance of object-centric
	      model based reinforcement learning in structured environments?
	\item Does the object-centric inductive bias in world models improve robustness to
	      changes in the environment?
\end{enumerate}

Within this thesis we propose to develop a world model that is running on top
of object centric states provided by the a framework called JAXAtari \cite{jaxatari}. This world model will be trained to predict 
the next
state of the environment given the current state and actionWithin. An actor-critic
framework based on the DreamerV2 \cite{hafner2020dream} architecture will be integrated with the world
model to learn policies from imagined rollouts. The training pipeline will
involve alternating between updating the world model and optimizing the policy
based on simulated experiences. The performance of the proposed approach will
be evaluated in the Pong environment from the Atari benchmark suite, comparing
it against pixel-based baselines and assessing its sample efficiency and
robustness. Combining both appraoches thus offers the possibility of complex planning
using object centric states. Although out of the scope of this thesis one could imagine interpretable
planned stategies describing how certain objects influence decisions of the agent. This is usually quite hard
with existing Model Based approaches since they usually rely on latent space which is then used to make predictions
resulting in the loss of human interpretability.

The remainder of this thesis is structured as follows: Chapter 2 provides
background information on reinforcement learning, model based approaches, and
object-centric representations. Chapter 3 details the methodology, including
the world model architecture, actor-critic integration, and training pipeline.
Chapter 4 presents the experimental setup, results, and analysis. Chapter 5
discusses the findings, challenges, and implications of the results. Finally,
Chapter 6 concludes the thesis and outlines directions for future work.

\chapter{Background and Related Work}
\label{chap:background}
This chapter aims to give an overview into the relevant mathematical background and related work
in the fields of Reinforcement Learning, Model Based Reinforcement Learning and Object-Centric Learning.
\section{Reinforcement Learning}
\label{sec:reinforcement_learning}

Reinforcement Learning (RL) is a machine learning subcategory that focuses on
training agents that make decision in an environment. An environment can be a
game, a robotic control task or any other sequential decision making problem.
To give that a more structured formulation the definition of a Markov Decision
Process (MDP) is used. This term comes from the field of operations research
and was named after the Russian mathematician Andrey Markov in relation to his
work on stochastic processes. An MDP is defined as a tuple $\mathcal{M} =
	(\mathcal{S}, \mathcal{A}, P, R, \gamma)$, where:

\begin{itemize}
	\item $\mathcal{S}$ is the state space
	\item $\mathcal{A}$ is the action space
	\item $P(s'|s,a)$ is the transition probability function defining the probability of transitioning to state $s'$ given current state $s$ and action $a$
	\item $R(s,a,s')$ is the reward function specifying the immediate reward received after taking action $a$ in state $s$ and transitioning to state $s'$
	\item $\gamma \in [0,1]$ is the discount factor that determines the importance of future rewards. This is also often used to prove convergence of certain algorithms.
\end{itemize}

Markov Chains assume the Markov property, which states that the future state
depends only on the current state and action, not on the entire history of past
states and actions. This memoryless property is what enabled many
computationally viable algorithms to be developed in the first place since
every probability computed is only conditioned on the current state and action
so a finite set of values instead of a potentially infinite set of past states
and actions.

Given that we now defined what the environment actually means let us examine at
the goal of the agent. The agent's goal is to learn an optimal policy
$\pi^*(s)$ that maximizes the expected cumulative discounted reward, known as
the return:
\begin{equation}
	G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\end{equation}

The value function $V^\pi(s)$ represents the expected return when starting from
state $s$ and following policy $\pi$:
\begin{equation}
	V^\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]
\end{equation}

In a similar manner, $Q^\pi(s,a)$ represents the expected return when starting
from state $s$, taking action $a$, and then following policy $\pi$. Making the
action value function a function of both state and action. This can sometimes
be more useful since it directly tells you the value of taking a certain action
as well.

Model free reinforcement learning approaches learn optimal policies directly
from experience without explicitly modeling the environment dynamics. These
methods estimate value functions or policies through trial-and-error
interactions with the environment, making them broadly applicable but
potentially sample-inefficient.

As mentioned in the introduction, model free rely on learning an agent directly
by interacting with the environment and receiving feedback in the form of
rewards. This trial-and-error process allows the agent to improve its policy
over time. By omitting a world model the complexity of the overall system is
reduced but the the agent is usually quite sample inefficient
\cite{duan2016benchmarking,kaiser2019model}. Also, the lack of a model can make
it difficult for the agent to plan ahead or consider the long-term consequences
of its actions. On top of that world models can improve generalization to new
tasks and make the agent more robust \cite{ha2018world,hafner2020dream}.
Moreover they provide limited interpretability with regards to their learned
policy.

Popular model free algorithms include temporal difference methods like Q-Learning
 \cite{watkins1989learning} and SARSA \cite{rummery1994line}, as well as policy 
 gradient methods such as REINFORCE \cite{williams1992simple}, Actor-Critic \cite{barto1983neuronlike}, 
 and Proximal Policy Optimization (PPO) \cite{schulman2017proximal}.

Model based reinforcement learning addresses some limitations of model free
approaches by learning an explicit model of the environment dynamics.

The world model typically learns to predict next states and rewards given
current states and actions:

\begin{align}
	\hat{s}_{t+1} & = f_\theta(s_t, a_t)        \\
	\hat{r}_{t+1} & = g_\phi(s_t, a_t, s_{t+1})
\end{align}

where $f_\theta$ and $g_\phi$ represent parameterized functions (often neural
networks) that approximate the true environment dynamics. Sometimes the reward
prediction can be omitted if the reward can somehow be directly read out from
the game itself which is common in Atari games for example assuming there
exists an object centric approach that encompasses the score.

Model based methods potentially have much better sample efficiency since it
takes less sampling in the environment to learn reasonable state transitions
than it takes to train a reinforcement learning. After the model has been
properly trained the agent can learn entirely within the worldmodel making
additional sampling unnecessary. Although depending on game complexity the
initial sampling which is usually done by a random policy might not see diverse
enough states. This is for most Atari Games not an issue but for more complex
environment it can be important to retrain the worldmodel with new experience
that is sampled by the now trained (within the model) trained agent in the real
environment \cite{janner2019trust,sutton1991dyna}. Moreover there are
approaches that use explicit planning strategies that hugely benefit
interpretability. Using the worldmodel the agent can make guesses what might
happen in the future and explicitly reason why it decides for a certain action
\cite{schrittwieser2020mastering,greydanus2018visualizing}.

But training a well designed and stable model can create several challenges for
example the world model can introduce a bias into the agent which doesnt hold
in the true environment. The agent can learn to exploit errors in the
worldmodel to artificially boost its score but then performing much worse
during inference \cite{talvitie2017self,lambert2020learning}.

\section{Dreamer Architecture}
\label{sec:dreamer_architecture}

The Dreamer architecture is a family of model based RL algorithms (DreamerV1,
V2, V3) that use a world model together with actor-critic methods. The main
idea is to learn a compact latent space where the agent can imagine rollouts
and train its policy without always interacting with the real environment.
Dreamer builds on ideas from PlaNet \cite{hafner2019learninglatentdynamicsplanning}, which first used latent dynamics, but
Dreamer adds actor-critic learning and works better for Atari games.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{images/DreamerFlow.png}
	\caption{Overview of the DreamerV2 training pipeline. The world model learns to predict future states and rewards, enabling the actor-critic framework to optimize policies using imagined rollouts. Taken from Hafner et al.~\cite{hafner2021mastering}, Fig.~1.}
	\label{fig:dreamer_flow}
\end{figure}

DreamerV2 improves DreamerV1 by making it more stable and able to handle
discrete actions, which is important for Atari. DreamerV3 tries to make the
method work for many different tasks without changing hyperparameters. There
are different ways to train a world model. You can predict future observations
directly or predict latent states. In Dreamers case as shown in Figure
\ref{fig:dreamer_flow} the world model predicts latent states.

DreamerV2 builds upon the world model with an actor-critic framework. The actor
is responsible for selecting actions based on the current latent state, while
the critic evaluates the chosen actions by estimating their value. Central to DreamerV2 is the
use of the $\lambda$-return, which provides a balance between bias and variance
since using $\lambda=0$ leads to high bias and low variance, while $\lambda=1$ leads to low bias and high variance. Approaching
TD(0) and Monte Carlo respectively. The following sections explain in detail how the
$\lambda$-target is computed and how it is used to define the actor and critic.

The $\lambda$-target is defined recursively as:
\begin{equation}
	V^{\lambda}_t = \hat{r}_t + \hat{\gamma}_t \cdot \begin{cases}
		(1 - \lambda)v_\xi(\hat{z}_{t+1}) + \lambda V^{\lambda}_{t+1} & \text{if } t < H \\
		v_\xi(\hat{z}_H)                                              & \text{if } t = H
	\end{cases}
\end{equation}

Here, $\hat{r}_t$ is the predicted reward, $\hat{\gamma}_t$ is the predicted
discount, $v_\xi(\hat{z}_t)$ is the critic value, and $\lambda$ (usually 0.95)
controls how much you care about future rewards. This formula mixes short and
long-term returns.

The actor in DreamerV2 uses a loss that mixes different gradient estimators to
make learning stable. The actor tries to maximize the same $\lambda$-returns as
the critic, using both REINFORCE and backpropagation through the model.

The actor loss is:
\begin{align}
	\mathcal{L}(\psi) = \mathbb{E}\left[\sum_{t=1}^{H-1} \Big[\right. & -\rho \ln p_\psi(\hat{a}_t | \hat{z}_t) \, \text{sg}(V^{\lambda}_t - v_\xi(\hat{z}_t)) \label{eq:reinforce} \\
	                                                                  & -(1-\rho)V^{\lambda}_t \label{eq:dynamics} \\
	                                                                  & -\eta \mathcal{H}[a_t|\hat{z}_t] \Big] \label{eq:entropy}
\end{align}

The loss has three parts:

\textbf{REINFORCE} \eqref{eq:reinforce}: Maximizes the log-probability of actions, weighted by the advantage (difference between lambda-return and critic). The stop-gradient (sg) is used to not mess up the critic.

\textbf{Dynamics Backprop} \eqref{eq:dynamics}: Backpropagates through the model for more direct gradients.

\textbf{Entropy Regularization} \eqref{eq:entropy}: Encourages exploration by making the action distribution more random. $\eta$ controls how much exploration.


$\rho$ decides how much REINFORCE vs. backprop you use. For Atari, usually $\rho=1$ (just REINFORCE), $\eta=10^{-3}$.

The critic in DreamerV2 estimates the expected return from a latent state. It
is needed for both the lambda-targets and as a baseline for REINFORCE.

The critic is trained with TD learning using the lambda-targets:
\begin{equation}
	\mathcal{L}(\xi) = \mathbb{E}\left[\sum_{t=1}^{H-1} \frac{1}{2} \left(v_\xi(\hat{z}_t) - \text{sg}(V^{\lambda}_t)\right)^2\right]
\end{equation}

Here, $v_\xi(\hat{z}_t)$ is the critic's value, and $\text{sg}(V^{\lambda}_t)$
is the lambda-target with stopped gradients. This helps keep learning stable.

Some tricks are used to make the critic better:

\textbf{Target Network:} Like in DQN \cite{DQN}, a delayed copy of the critic is used for targets, updated every 100 steps.

\textbf{Trajectory Weighting:} Loss terms are weighted by predicted discounts, so episode ends are handled properly.

\textbf{Latent States:} The critic works on compact latent states, not raw observations, which makes learning faster and more stable.

The critic is usually a small MLP with ELU activations and about 1 million
parameters, outputting a single value for each state.

Given the advantages of model based RL and the effectiveness of the Dreamer
architecture we now come to the question of how to best represent the state of
the environment.

\section{Object-Centric Reinforcement Learning}
\label{sec:object_centric_rl}

Object-Centric Reinforcement Learning (OCRL) is a collection of methods that do
not make predictions on the raw pixel input but rather try to first create an
abstract representation of the environment which has some semantic
understanding of the objects within it. There are methods that use the pixel
input to create objects from it \cite{locatello2020object} but also approaches
where the objects states are extracted from RAM \cite{delfosse2023ocatari} or
given directly by the environment itself like in JAXAtari.

This object extraction mimics human perception, which naturally segments scenes
into distinct entities and focuses on their interactions. Therefore object centricity provides several advantages over pixel based
methods.

\textbf{Compositional Understanding:} Object-centric representations naturally excel at treating input features as compositions of distinct entities.
This compositionality allows agents to reason about individual objects and their interactions by for example allowing the agent
to make connections between velocity and the position of a ball to a paddle.

\textbf{Improved Generalization:} By focusing on objects rather than pixel patterns, agents can generalize better across visually different but similar scenarios. For example agents that
understands the concept of a "ball" an an abstract object can use this knowledge in environments where the ball has a different shape or appearance in general \cite{zambaldi2018deep}.

\textbf{Enhanced Interpretability:} Object-centric representations provide naturally a better interpretability than pixel based representations since the agent can reason better that it perceives things as objects rather than "random" pixels.

\textbf{Sample Efficiency:} The structured nature of object-centric representations often leads to more sample-efficient learning. By working with compact, meaningful features rather than high-dimensional
pixel data, agents can learn policies with fewer rollouts. This is possible because the object space is already some kind of latent space which is usually the way things work \cite{locatello2020object}.

A significant development in this field is the introduction of OCAtari
(Object-Centric Atari) by Delfosse et al. \cite{delfosse2023ocatari}. OCAtari
extends the widely-used Arcade Learning Environment (ALE) by providing
resource-efficient extraction of object-centric states for Atari 2600 games.
This framework fixes a gap, where despite growing interest in object-centric
approaches, no standardized benchmark existed for evaluating such methods on
the popular Atari domain.

The OCAtari framework works by two ways of extracting object states. It can
either extract the object states directly from the emulator's RAM or by using
template matching on the rendered frames. The RAM-based extraction is more
efficient and accurate. Directly having those extracted object states allows
for significantly faster training times and also supports researches in making
comparable findings since everyone can use the same object states.

The importance of object-centric approaches in Atari environments is especially
present in light of pixel-based methods in these domains. Delfosse et al.
demonstrated that deep RL agents without interpretable object-centric
representations can learn misaligned policies even in simple games like Pong
\cite{delfosse2023ocatari}. This misalignment problem strengthens the argument
for using object-centric understanding rather than attempting to retrofit
interpretability onto pixel-based systems.

Building upon OCAtari, JAXAtari \cite{machado2023revisiting} provides a
JAX-based implementation that offers additional performance benefits through
just-in-time compilation and vectorized environments. JAXAtari maintains
compatibility with the object-centric state extraction capabilities of OCAtari
while providing enhanced computational efficiency for large-scale experiments.
This framework is particularly relevant for our work as it enables efficient
training of world models on object-centric representations while maintaining
the standardized benchmarking capabilities essential for reproducible research.

Object-centric representations naturally lend themselves to relational
reasoning, where agents must understand not just individual objects but also
the relationships and interactions between them. This capability is crucial for
complex decision-making in multi-object environments where the optimal policy
depends on understanding how different entities influence each other.

Relational reasoning in reinforcement learning encompasses several key aspects:

\textbf{Spatial Relationships:} Understanding the relative positions of objects and how spatial configurations affect optimal actions. For example, in Pong, the relationship between the paddle position
and ball trajectory determines the appropriate movement strategy.

\textbf{Temporal Relationships:} Tracking how object relationships evolve over time and predicting future interactions. This temporal aspect is particularly important for planning and anticipatory behavior.

\textbf{Causality:} \cite{scholkopf2021toward,dasgupta2019causal} Recognizing cause-and-effect relationships between actions and object state changes. This understanding enables more sophisticated planning and can help avoid unintended consequences.



Object-centric model based RL combines the abstraction of object representations
with state prediction. By using structured object-centric states, agents can
reason about state transitions in a more robust and interpretable way compared
to pixel-based representations. Pixel based representation can thus suffer from
several issues namely:

\begin{itemize}
	\item \textbf{High Dimensionality:} Pixel based inputs are by nature more prone to having more dimensions, making them harder to process and requiring more computational resources.
	\item \textbf{Poor Generalization:} Agents trained on pixel data may struggle to generalize across different environments or object appearances, as they rely heavily on specific visual features and are very prone to slight changes that would not affect human performances but greatly affects the agent \cite{zhang2020learning,stone2021distracting}.
	\item \textbf{Lack of Semantic Understanding:} Pixel representations do not inherently capture the relationships between objects, limiting the agent's ability to reason about their interactions.
	\item \textbf{Sample Inefficiency:} Learning from high-dimensional pixel data often requires more samples to achieve comparable performance to object-centric approaches \cite{watters2019cobra}.
	\item \textbf{Sensitivity to Noise and Distractors:} Pixel-based methods sometimes focus on irrelevant visual details of the environment which get abstracted away when using object-centric approaches.
\end{itemize}


The integration of relational reasoning with model based approaches offers
significant potential for improving agent performance. By incorporating
relational structure into world models, agents can make more accurate
predictions about future states and plan more effectively. This integration
forms a core component of our proposed approach, where object-centric world
models explicitly encode relational information to enhance both prediction
accuracy and policy learning.

\chapter{Methodology}
\label{chap:methodology}

This chapter presents the technical details of our object-centric model based reinforcement learning approach for Pong. We describe the world model architecture, the actor-critic framework, and the complete training pipeline that integrates these components.

\section{World Model Architecture}
\label{sec:world_model_arch}

Our world model is designed to predict future object-centric states given the current state and action. Unlike pixel-based approaches that operate on high-dimensional visual inputs, our model works with compact object representations provided by JAXAtari, consisting of 48 features (12 object features across 4 stacked frames, with score features removed to avoid trivial prediction).

\subsection{Deep Residual MLP (PongMLPDeep)}
\label{subsec:pongmlp}

Our world model \texttt{PongMLPDeep} is a deep feedforward neural network with residual connections and layer normalization. Instead of using recurrent architectures, we rely on frame stacking to provide the temporal context. The input state contains 4 consecutive frames, which gives the model information about velocities and trajectories through the temporal differences between frames.

The feedforward architecture offers several advantages for our object-centric setting. First, frame stacking provides sufficient temporal information for the deterministic physics of Pong—the ball's velocity and direction can be inferred from the consecutive frame differences. Second, feedforward networks are significantly faster to train and evaluate than recurrent models, since they have no sequential dependencies that require iterative computation. Third, the structured nature of object-centric states (explicit positions, velocities) means the model does not need to learn temporal abstractions from the scratch.

Our implementation uses a 4-layer residual architecture that predicts frame-shifted states. The model is defined as:

\begin{equation}
\hat{s}_{t+1} = f_\theta(s_t, a_t)
\end{equation}

where $s_t \in \mathbb{R}^{48}$ is the normalized object-centric state containing 4 stacked frames, $a_t \in \{0,1,2,3,4,5\}$ is the discrete action, and $f_\theta$ represents our parameterized dynamics function. Importantly, the output $\hat{s}_{t+1}$ represents a shifted frame stack: frames $[f_1, f_2, f_3, f_{\text{new}}]$ where the oldest frame $f_0$ gets discarded and a new predicted frame gets appended.

The architecture consists of following layers with hidden dimension $h = 256 \cdot \text{scale\_factor}$:

\begin{enumerate}
    \item \textbf{Input Processing:} State and one-hot encoded action are concatenated: $x_0 = [s_t; \text{OneHot}(a_t)] \in \mathbb{R}^{54}$
    \item \textbf{Layer 1:} $x_1 = \text{GELU}(\text{LayerNorm}(\text{Dense}_h(x_0)))$
    \item \textbf{Layer 2 with Residual:} $x_2 = \text{GELU}(\text{LayerNorm}(\text{Dense}_h(x_1))) + x_1$
    \item \textbf{Layer 3 with Residual:} $x_3 = \text{GELU}(\text{LayerNorm}(\text{Dense}_h(x_2))) + x_2$
    \item \textbf{Layer 4 with Residual:} $x_4 = \text{GELU}(\text{LayerNorm}(\text{Dense}_h(x_3))) + x_3$
    \item \textbf{Output Layer:} Predict only the new frame: $f_{\text{new}} = \text{Dense}_{12}(x_4)$
    \item \textbf{Frame Shifting:} Construct $\hat{s}_{t+1}$ by shifting frames: $[f_1, f_2, f_3, f_{\text{new}}]$
\end{enumerate}

The residual connections after layers 2-4 facilitate the gradient flow through the deep network and prevent vanishing gradients. Layer normalization after each dense layer stabilizes the activations and accelerates the training. We use GELU (Gaussian Error Linear Unit) activation functions, which have been shown to work well for deep networks compared to ReLU.

An important architectural choice is to predict only the newest frame instead of the entire 48-dimensional state. This reduces the output complexity from 48 to 12 dimensions and naturally incorporates an inductive bias: the model focuses on predicting how objects change instead of reconstructing static features from previous frames. The frame shifting operation (model\_architectures.py:77-107) handles the interleaved feature format where each of 12 features has 4 frame values.

\subsection{Alternative Architectures Explored}
\label{subsec:alternative_architectures}

Before settling on the deep residual MLP architecture, we explored several alternative approaches to understand their trade-offs for object-centric world modeling:

\textbf{Shallow MLP:} A lightweight feedforward architecture with 2 hidden layers of 128 neurons each. While being computationally efficient and fast to train, shallow networks struggled with modeling the complex state transitions. The limited capacity meant that the model could not capture the nonlinear interactions between objects (e.g., ball-paddle collisions with varying angles) accurately.

\textbf{MLP without Residual Connections:} We tested deep architectures (4-6 layers) without skip connections. These models showed training difficulties due to vanishing gradients, especially in the deeper variants. The convergence was slow and the final performance was inferior to the residual version, which confirms the importance of residual connections for deep feedforward networks.

\textbf{Recurrent Architectures (LSTM/GRU):} We considered LSTM and GRU networks that maintain hidden states across the time. While recurrent models can theoretically learn more rich temporal dynamics, they introduce significant computational overhead due to the sequential dependencies. Each prediction requires to iterate through LSTM cells, which prevents efficient parallelization. Moreover, for the deterministic physics of Pong, frame stacking provides sufficient temporal context without the need for learned memory.

In preliminary experiments, LSTM-based models showed comparable single-step prediction accuracy but were 3-5× slower during the imagined rollout generation—which is a critical bottleneck since we generate 30,000 rollouts per training iteration. The computational cost outweighed any potential benefits for our structured, low-dimensional state space.

\textbf{Transformer-based Models:} We considered also transformer architectures with self-attention over the frame history. Transformers excel at learning long-range dependencies from large datasets, but our setting does not require these capabilities. The physics of Pong are local and Markovian—the next state depends primarily on the most recent 2-3 frames. Additionally, transformers have higher parameter counts and data requirements, which conflicts with our sample efficiency goals.

\textbf{Direct State Prediction vs. Frame Prediction:} An important design choice was whether to predict the full 48-dimensional next state or only the newest 12-dimensional frame. We found that the frame-based prediction (our final choice) works better because of:
\begin{itemize}
\item \textbf{Reduced output complexity:} 12 outputs vs. 48 reduces the model capacity requirements
\item \textbf{Inductive bias:} Forces the model to predict changes instead of copying static features
\item \textbf{Better generalization:} Less prone to overfitting when the training data is limited
\end{itemize}

Ultimately, we selected the 4-layer residual MLP with frame prediction since it provides the best balance between prediction accuracy, computational efficiency, and training stability. With hidden dimension 256 (scale\_factor=1), the model has approximately 600K parameters—which is sufficient for the structured dynamics of Pong while remaining tractable for training on limited experience data.

\subsection{State Normalization and Stability}
\label{subsec:normalization}

Normalization is crucial for a stable training in deep reinforcement learning. Object-centric states in Pong have features with very different scales: paddle positions range from 0-190, ball positions from 0-160 (x) and 0-210 (y), while velocities range from approximately -3 to 3. Without normalization, the gradients would be dominated by the high-magnitude features, which leads to unstable training and poor convergence.

We apply Z-score normalization to all state features:

\begin{equation}
\tilde{s}_t = \frac{s_t - \mu}{\sigma + \epsilon}
\end{equation}

where $\mu \in \mathbb{R}^{48}$ and $\sigma \in \mathbb{R}^{48}$ are the mean and standard deviation computed over the training dataset, and $\epsilon = 10^{-8}$ prevents the division by zero. The normalization statistics are computed only from valid training transitions (excluding the episode boundaries) and get saved with the model checkpoint for the consistent inference.

During training, we normalize the inputs before feeding them to the network, and denormalize the predictions back to the original scale:

\begin{equation}
\hat{s}_{t+1} = \tilde{s}_{t+1} \cdot \sigma + \mu
\end{equation}

For the world model training, we additionally apply feature-specific loss weighting to emphasize the physically critical variables:

\begin{equation}
\mathcal{L}_{\text{world}} = \mathbb{E}\left[\sum_{i=1}^{48} w_i \left(\hat{s}_{t+1}^{(i)} - s_{t+1}^{(i)}\right)^2\right]
\end{equation}

where the weights are set as: $w_i = 10.0$ for ball position features (indices 32-39), $w_i = 5.0$ for ball velocity features (indices 16-23), $w_i = 2.0$ for paddle position (indices 4-7), and $w_i = 1.0$ for other features. This weighting scheme reflects that the ball dynamics are most critical for accurate long-term rollouts, since the position errors in the ball compound fastest during the imagined trajectories.

Additional stability measures include:

\begin{itemize}
    \item \textbf{Gradient Clipping:} Global norm clipping at 1.0 prevents gradient explosion in deep networks
    \item \textbf{Learning Rate:} Conservative Adam optimizer with $\alpha = 3 \times 10^{-4}$ and $\epsilon = 10^{-5}$
    \item \textbf{Residual Connections:} Skip connections in layers 2-4 facilitate gradient flow through the 4-layer network
    \item \textbf{Layer Normalization:} Applied after each dense layer to stabilize activations and accelerate convergence
    \item \textbf{GELU Activation:} Gaussian Error Linear Units provide smoother gradients than ReLU, improving training stability
\end{itemize}

\section{Actor-Critic Integration}
\label{sec:actor_critic}

Following the DreamerV2 framework, we train an actor-critic policy entirely from the imagined rollouts generated by our object-centric world model. Unlike the original DreamerV2 which operates on latent states from a learned image encoder, our approach directly uses the interpretable object-centric states from JAXAtari. This architectural choice eliminates the need for the representation learning while maintaining the sample efficiency benefits of model based RL.

\subsection{DreamerV2-Style Actor-Critic}
\label{subsec:dreamer_ac}

The key insight of the Dreamer approach is to separate the representation learning from the policy optimization. DreamerV2 learns a latent world model that compresses the high-dimensional observations into compact latent states, then trains actor-critic networks purely in this imagined latent space. Our approach differs in that we bypass the representation learning phase entirely—the object-centric states from JAXAtari already provide a compact, meaningful representation of the environment.

\textbf{Actor Network:} The actor $\pi_\psi(a|s)$ is a categorical distribution over the 6 discrete actions in Pong (NOOP, UP, DOWN, and their variants). It is implemented as a 3-layer MLP with 64 hidden units per layer, ELU activations, and a final softmax output:

\begin{align}
h_1 &= \text{ELU}(\text{Dense}_{64}(s)) \\
h_2 &= \text{ELU}(\text{Dense}_{64}(h_1)) \\
h_3 &= \text{ELU}(\text{Dense}_{64}(h_2)) \\
\pi_\psi(a|s) &= \text{Categorical}(\text{Dense}_6(h_3))
\end{align}

The network uses orthogonal initialization with scaling $\sqrt{2}$ for the hidden layers and 0.01 for the output layer, which promotes stable gradient flow and prevents the premature convergence to deterministic policies.

\textbf{Critic Network:} The critic $v_\xi(s)$ estimates the state values through a distributional representation. Instead of outputting a single scalar value, it predicts a Gaussian distribution $\mathcal{N}(\mu_v, \sigma_v)$ over the returns:

\begin{align}
h_1 &= \text{ELU}(\text{Dense}_{64}(s)) \\
h_2 &= \text{ELU}(\text{Dense}_{64}(h_1)) \\
h_3 &= \text{ELU}(\text{Dense}_{64}(h_2)) \\
\mu_v &= \text{Dense}_1(h_3) \\
\log \sigma_v &= \text{clip}(\text{Dense}_1(h_3), -3, 0)
\end{align}

The distributional critic provides the uncertainty estimates and has been shown to improve the learning stability in actor-critic methods. During training, we minimize the negative log-likelihood of the target returns under this distribution.

\subsection{Policy Learning in Imagined Rollouts}
\label{subsec:imagined_rollouts}

A core principle of model based RL is to learn policies from the simulated trajectories instead of requiring expensive real environment interactions. Our training procedure operates in two phases:

\textbf{Phase 1: Rollout Generation}

Given a batch of $N=30000$ initial states sampled from the experience buffer, we generate the imagined trajectories of length $H=7$ by repeatedly applying the current actor policy in the world model:

\begin{algorithm}
\caption{Imagined Rollout Generation}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Initial states $\{s_0^{(i)}\}_{i=1}^N$, world model $f_\theta$, actor $\pi_\psi$, critic $v_\xi$
\STATE \textbf{Output:} Trajectories $(s_t, a_t, r_t, \gamma_t, v_t)$ for $t=0..H$
\FOR{$i = 1$ to $N$}
    \STATE $s_t \gets s_0^{(i)}$ \hfill // Initialize from experience buffer
    \FOR{$t = 0$ to $H-1$}
        \STATE Sample action: $a_t \sim \pi_\psi(\cdot | s_t)$
        \STATE Predict value: $v_t \gets \mathbb{E}[v_\xi(s_t)]$
        \STATE Predict next state: $s_{t+1} \gets f_\theta(s_t, a_t)$
        \STATE Compute reward: $r_t \gets R(s_{t+1}, a_t)$
        \STATE Set discount: $\gamma_t \gets 0.95$
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

The rollout length $H=7$ was chosen to balance the computational efficiency with sufficient lookahead for the credit assignment. Shorter rollouts ($H < 5$) provided insufficient temporal context, while longer rollouts ($H > 10$) accumulated the prediction errors that degraded the policy learning.

Critically, the rewards are computed from the predicted states using a hand-crafted reward function instead of relying on the environment's score. We use an improved reward function based on the ball-paddle interactions:

\begin{equation}
R(s_{t+1}, a_t) = \begin{cases}
+2.0 & \text{if ball crosses left edge and paddle aligned} \\
-2.0 & \text{if ball crosses left edge and paddle misaligned} \\
+0.5 & \text{if ball moving toward paddle and paddle tracking} \\
-0.1 & \text{if paddle moving without ball approaching} \\
0.0 & \text{otherwise}
\end{cases}
\end{equation}

This dense reward provides more learning signal than the sparse score-based rewards, especially during the early training when the agent rarely hits the ball.

\textbf{Phase 2: Policy Optimization}

Once the imagined rollouts are generated, we train the actor and critic using the collected trajectories. The critic is trained to predict the lambda-returns (described in Section \ref{subsec:lambda_returns}), while the actor maximizes these predicted returns using the REINFORCE gradient estimator.

A key implementation detail is the batching strategy. We flatten all trajectories into a single dataset of $(s_t, a_t, V^\lambda_t)$ tuples totaling $N \times H = 210,000$ samples, then we perform mini-batch SGD with batch size 256. This approach improves the hardware utilization and gradient stability compared to the per-trajectory updates.

\subsection{Lambda-Return Computation}
\label{subsec:lambda_returns}

The lambda-return $V^\lambda_t$ provides a bias-variance trade-off between the Monte Carlo returns (high variance, low bias) and the TD(0) estimates (low variance, high bias). We compute the lambda-returns recursively following the DreamerV2 formulation:

\begin{equation}
V^\lambda_t = r_t + \gamma_t \cdot \left[(1 - \lambda) v_\xi(s_{t+1}) + \lambda V^\lambda_{t+1}\right]
\label{eq:lambda_return}
\end{equation}

with the base case $V^\lambda_H = v_\xi(s_H)$ at the horizon. We set $\lambda = 0.95$ following the DreamerV2's Atari configuration, which heavily weights the bootstrapped value estimates while maintaining some Monte Carlo influence.

The computation is implemented via a JAX scan operation that processes the trajectories backwards in time:

\begin{algorithm}
\caption{Lambda-Return Computation (Backward Pass)}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Rewards $\{r_t\}_{t=1}^{H}$, values $\{v_t\}_{t=0}^{H}$, discounts $\{\gamma_t\}_{t=1}^{H}$, $\lambda$
\STATE \textbf{Output:} Lambda-returns $\{V^\lambda_t\}_{t=0}^{H-1}$
\STATE $V^\lambda_H \gets v_H$ \hfill // Bootstrap from final value
\FOR{$t = H-1$ down to $0$}
    \STATE $V^\lambda_t \gets r_{t+1} + \gamma_{t+1} \left[(1-\lambda) v_{t+1} + \lambda V^\lambda_{t+1}\right]$
\ENDFOR
\STATE \textbf{return} $\{V^\lambda_t\}_{t=0}^{H-1}$
\end{algorithmic}
\end{algorithm}

These lambda-returns serve as regression targets for the critic:

\begin{equation}
\mathcal{L}_{\text{critic}} = \mathbb{E}_{s \sim \text{rollouts}}\left[-\log p_{v_\xi}(V^\lambda | s)\right]
\end{equation}

where $p_{v_\xi}(V^\lambda | s)$ is the Gaussian density predicted by the critic. The stop-gradient operator is applied to the lambda-returns to prevent the gradients from flowing back through the target computation.

The actor is trained using the policy gradient with advantages computed as $A_t = V^\lambda_t - v_\xi(s_t)$:

\begin{equation}
\mathcal{L}_{\text{actor}} = \mathbb{E}_{s,a \sim \text{rollouts}}\left[-\log \pi_\psi(a|s) \cdot \text{sg}(A_t) - \eta \mathcal{H}[\pi_\psi(\cdot|s)]\right]
\end{equation}

where $\eta = 0.01$ is the entropy bonus coefficient that encourages exploration, and $\text{sg}(\cdot)$ denotes the stop-gradient. We additionally normalize the advantages to have zero mean and unit variance within each batch, which improves the gradient stability.

\section{Training Pipeline}
\label{sec:training_pipeline}

Our complete training procedure alternates between the world model training and the policy optimization, with periodic retraining of the world model using fresh experience collected by the improving policy. This cyclic approach addresses a key challenge in model based RL: as the policy improves, it explores new regions of the state space where the world model may be inaccurate.

\subsection{World Model Training and Experience Collection}
\label{subsec:world_model_training}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.3\textwidth]{images/trainingschedule.png}
	\caption{Training schedule showing the alternating phases of world model training and policy optimization. The world model is retrained every 10 policy iterations using newly collected experience from the current policy.}
	\label{fig:trainingschedule}
\end{figure}

The training pipeline begins with an initial data collection phase, followed by iterative improvement of both the world model and policy:

\begin{algorithm}
\caption{Complete Training Pipeline}
\begin{algorithmic}[1]
\STATE \textbf{// Phase 1: Initial Setup}
\IF{no experience buffer exists}
    \STATE Collect 160,000 steps using ball-tracking policy in real environment
    \STATE Train world model $f_\theta$ for 50 epochs on collected data
\ENDIF
\STATE Load world model parameters $\theta$ and experience buffer
\STATE Initialize actor $\pi_\psi$ and critic $v_\xi$ with random weights
\STATE
\STATE \textbf{// Phase 2: Iterative Training}
\FOR{iteration $i = 0$ to 3120}
    \STATE \textbf{// Policy optimization from imagination}
    \STATE Sample 30,000 initial states from experience buffer
    \STATE Generate $H=7$ step imagined rollouts using $f_\theta$, $\pi_\psi$
    \STATE Compute lambda-returns using $v_\xi$
    \STATE Update $\pi_\psi$ and $v_\xi$ for 10 epochs on imagined data
    \STATE
    \STATE \textbf{// Evaluation}
    \IF{$i \mod 10 = 0$}
        \STATE Evaluate $\pi_\psi$ for 50 episodes in real environment
        \IF{mean reward $>$ best so far}
            \STATE Save best checkpoint
        \ENDIF
    \ENDIF
    \STATE
    \STATE \textbf{// World model retraining (for model-based mode only)}
    \IF{$i \mod 10 = 0$ and using model-based rollouts}
        \STATE Collect 160,000 new steps using current $\pi_\psi$ in real environment
        \STATE Retrain $f_\theta$ for 50 epochs on updated experience buffer
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\textbf{Initial Experience Collection:} Before any training begins, we collect a diverse dataset of state transitions using a ball-tracking heuristic policy. This policy moves the paddle toward the ball's y-coordinate with 50\% random exploration, which provides reasonable coverage of reachable states without requiring a pre-trained agent. The ball-tracking policy achieves approximately -5 to +5 average reward in Pong, which is sufficient for observing the ball-paddle interactions.

The collected experience is stored as tuples $(s_t, a_t, s_{t+1})$ with metadata marking the episode and life boundaries (ball resets). These boundaries are crucial for the world model training—we exclude the transitions that cross boundaries from the training batches, since they violate the temporal continuity.

\textbf{World Model Training Details:} The world model is trained via supervised learning to predict the one-step transitions. We use the mean squared error loss with feature-specific weighting:

\begin{equation}
\mathcal{L}_{\text{world}} = \mathbb{E}_{(s,a,s') \sim \mathcal{D}}\left[\sum_{i=1}^{48} w_i \left(f_\theta(s, a)^{(i)} - s'^{(i)}\right)^2\right]
\end{equation}

Training uses the Adam optimizer with learning rate $3 \times 10^{-4}$, batch size 512, and 50 epochs. The model gets saved every 10 epochs along with the normalization statistics.

\textbf{Experience Buffer Management:} The experience buffer maintains a sliding window of recent transitions. In the real-environment baseline mode, we do not update the buffer after the initial collection. In the model-based mode, we periodically collect fresh data using the current policy, then retrain the world model every 10 policy optimization iterations.

When retraining, we append new transitions to the existing buffer instead of replacing it. This prevents the catastrophic forgetting of state regions that the improved policy no longer visits. The buffer size is capped at 500,000 transitions to manage the memory.

\subsection{Policy Optimization}
\label{subsec:policy_optimization}

Within each training iteration, the policy optimization proceeds through several stages:

\textbf{Stage 1: Rollout Generation:} We sample 30,000 initial states uniformly from the experience buffer and generate the 7-step imagined trajectories. The large batch size improves the GPU utilization and provides diverse training signal. The rollouts are generated in sub-batches of 100 to manage the memory, with each sub-batch processed in parallel via the JAX's vmap and scan primitives.

\textbf{Stage 2: Data Preparation:} The generated trajectories are flattened into a dataset of 210,000 $(s_t, a_t, r_t, \gamma_t, v_t)$ tuples. We compute the lambda-returns via a vectorized backward pass over all trajectories simultaneously. The advantages are then computed and normalized.

\textbf{Stage 3: Actor-Critic Updates:} We perform 10 epochs of mini-batch gradient descent on the prepared data. Each epoch shuffles the data and processes it in batches of 256. The critic and actor get updated sequentially within each mini-batch:

\begin{align}
\xi &\gets \xi - \alpha_{\text{critic}} \nabla_\xi \mathcal{L}_{\text{critic}} \\
\psi &\gets \psi - \alpha_{\text{actor}} \nabla_\psi \mathcal{L}_{\text{actor}}
\end{align}

with learning rates $\alpha_{\text{critic}} = 5 \times 10^{-4}$ and $\alpha_{\text{actor}} = 8 \times 10^{-5}$. Both optimizers use the gradient clipping at global norm 0.5 to prevent the instability.

\textbf{Early Stopping:} We employ two early stopping criteria: (1) KL divergence threshold—if $D_{KL}(\pi_{\text{old}} || \pi_{\text{new}}) > 0.15$, we terminate the updates early. (2) Loss plateau—if the combined loss does not improve for 100 consecutive epochs, we reduce the actor learning rate by 50\%.

The complete policy optimization cycle takes approximately 95 seconds per iteration on an NVIDIA RTX 3090 GPU, which enables 3120 iterations in roughly 80 hours of wall-clock time.

\chapter{Experiments and Results}
\label{chap:experiments}
(QUESTION Soll ich hier Experimente 1,2,3 etc. auflisten und die dann auf die Forschungsfragen mappen
z.B. Experiment 1 10 Schritte unrollst und real vs model vs oc-model bilder -> vielleicht hier einfach
mein OC State Rendern und dann irgendeinen Bild Loss wählen, Dafür muss ich aber ein pixel based world model erstmal haben (einfach ein MLP hinzimmern?))
(Normalen Dreamer zum laufen bringen auch für RQ3 Farbe vom Ball ändern)
(Ablation Studies: Unterschiedliche Architekturen, Unterschiedliche Reward Funktionen)

\section{Experimental Setup}
\label{sec:exp_setup}
* Experimental Design: Baseline, Comparisons, Eval Metrics
* Environment and Setup: Pong Environment Characteristics, Object-Centric State Description
* Hyperparameters and Configuration: Model Architectures, Training Regimes
* Hardware and Implementation Details: Computational Resources, Software Frameworks
* Evaluation Protocol: Training and Testing Procedures, Statistical Analysis

\section{World Model Performance}
\label{sec:world_model_perf}

\subsection{Prediction Accuracy Analysis}
\label{subsec:prediction_accuracy}
Answer here : (RQ1) How does the prediction accuracy of object-centric world models compare to pixel-based world models in terms of state transition prediction and long-term rollout quality in the Pong environment?

\subsection{Long-Term Rollout Quality}
\label{subsec:rollout_quality}
Answer here : (RQ1) How does the prediction accuracy of object-centric world models compare to pixel-based world models in terms of state transition prediction and long-term rollout quality in the Pong environment?

\subsection{Model Stability Assessment}
\label{subsec:stability}
Answer here : (RQ1) How does the prediction accuracy of object-centric world models compare to pixel-based world models in terms of state transition prediction and long-term rollout quality in the Pong environment?

\section{Policy Learning Results}
\label{sec:policy_results}

\subsection{Sample Efficiency Comparison}
\label{subsec:sample_efficiency_comp}
Answer here : (RQ1) To what extent does integrating object-centric representations with model based RL improve sample efficiency compared to model free baselines and pixel-based model based approaches?

\subsection{Final Performance Evaluation}
\label{subsec:final_performance}
Answer here : (RQ1) To what extent does integrating object-centric representations with model based RL improve sample efficiency compared to model free baselines and pixel-based model based approaches?

\subsection{Learning Curve Analysis}
\label{subsec:learning_curves}
Answer here : (RQ1) To what extent does integrating object-centric representations with model based RL improve sample efficiency compared to model free baselines and pixel-based model based approaches?

\section{Ablation Studies}
\label{sec:ablation_studies}

\subsection{Impact of Object-Centric Representations}
\label{subsec:oc_impact}
Answer here : (RQ2) How effectively can the DreamerV2-style actor-critic framework learn optimal policies when trained on imagined rollouts from object-centric world models?

\subsection{Architecture Component Analysis}
\label{subsec:architecture_analysis}
Answer here : (RQ3) What is the impact of different world model architectures (LSTM-based vs. alternatives) on the overall performance of object-centric model based RL in structured environments?

\subsection{Reward Function Design Effects}
\label{subsec:reward_effects}
Answer here : (RQ3) What is the impact of different world model architectures (LSTM-based vs. alternatives) on the overall performance of object-centric model based RL in structured environments?

\subsection{Real vs. Model Comparison}
\label{subsec:real_vs_model}
Answer here : (RQ2) How effectively can the DreamerV2-style actor-critic framework learn optimal policies when trained on imagined rollouts from object-centric world models?

\subsection{Policy Behavior Visualization}
\label{subsec:policy_visualization}
Answer here : (RQ2) How effectively can the DreamerV2-style actor-critic framework learn optimal policies when trained on imagined rollouts from object-centric world models?

\chapter{Discussion}
\label{chap:discussion}
* Challenges during training, what decreases model performance

* Analysis of Results
- Strengths of the Proposed Approach
- Limitations and Challenges
- Comparison with Existing Methods

* Technical Insights
- World Model Design Choices
- Training Stability Issues
- Reward Engineering Importance

* Implications for Object-Centric RL
- Benefits of Integration
- Generalization Potential
- Scalability Considerations

* Future Research Directions
- More Complex Environments
- Improved Object Discovery
- Multi-Object Scenarios

\chapter{Conclusion and Future Work}
\label{chap:conclusion}

\begin{itemize}
	\item Summary of Contributions
	\item Key Findings
	\item Limitations and Future Work
	\item Final Remarks
\end{itemize}

\printbibliography[title={References}]

\appendix

\chapter{Implementation Details}
\label{app:implementation}

\section{Code Structure and Organization}
\label{app:code_structure}
Provide an overview of the project's codebase, including the main modules, their purposes, and how they interact.

\section{Hyperparameter Sensitivity Analysis}
\label{app:hyperparameter_analysis}
Discuss the impact of varying key hyperparameters on model performance and training stability.

\section{Additional Experimental Results}
\label{app:additional_results}
Include supplementary experimental results that support the main findings, such as extended comparisons or detailed metrics.

\chapter{Technical Specifications}
\label{app:technical_specs}

\section{Hardware Requirements}
\label{app:hardware_requirements}
List the hardware specifications required to reproduce the experiments, including GPU/CPU details and memory requirements.

\section{Software Dependencies}
\label{app:software_dependencies}
Detail the software libraries, frameworks, and versions used in the implementation.

\section{Reproducibility Guidelines}
\label{app:reproducibility}
Provide step-by-step instructions for reproducing the experiments, including setup, data preparation, and execution.

\chapter{Supplementary Figures and Tables}
Include additional figures and tables that complement the main text, such as
visualizations of training curves or ablation study results.
\label{app:supplementary}

\end{document}